{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1 \n",
    "### Introduction to the problem/task and dataset\n",
    "\n",
    "PumpkinSeed-ML-Insights is a comprehensive repository dedicated to the exploration and analysis of Turkish pumpkin seed varieties, with a focus on classifying whether a seed belongs to Urgup Sivrisi or Cercevelik species. This project demonstrates the knowledge of authors in data science and machine learning.\n",
    "\n",
    "Within this repository, you will find a Jupyter Notebook that serves as a self-explanatory document, guiding you through the entire process. This repository also contains three Python files, each implementing a different machine learning model: `knn_pumpkinseed.py`, `logistic_regression_pumkinseed.py`, and `neural_network_pumpkinseed.py`. There is also the `pumpkin_seeds.csv`, which contains the data and `Pumpkin_seeds.pdf`, which contains some description of the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 2\n",
    "### Description of the dataset\n",
    "\n",
    "`pumpkin_seeds.csv` is a CSV file containing information about Pumpkin Seeds found in Turkey. \n",
    "\n",
    "This dataset came from the study `The use of machine learning methods in classification of pumpkin seeds (Cucurbita pepo L.).` by Koklu, M., Sarigil, S., & Ozbek, O. in 2021. In their paper, they used a product shooting box to obtain quality images of the pumpkin seeds. The authors converted the images to a gray tone and then to binary images. To convert the image data into a CSV file, they extracted 12 morphological features.\n",
    "\n",
    "Overall, the CSV file has 13 columns/features and 2500 rows. The first 12 columns are from the extracted morphological features, while the last column classifies whether it belongs to the Urgup Sivrisi or Cercevelik species. There are 2500 rows, representing a single seed used in the study. There are 1200 Urgup Sivrisi and 1300 Cercevelik species of pumpkin seeds. \n",
    "\n",
    "The features found in this CSV file are as follows:\n",
    "1. Area ‚Äì Number of pixels within the borders of a pumpkin seed\n",
    "2. Perimeter ‚Äì Circumference in pixels of a pumpkin seed\n",
    "3. Major_Axis_Length ‚Äì Large axis distance of a pumpkin seed\n",
    "4. Minor_Axis_Length ‚Äì Small axis distance of a pumpkin seed\n",
    "5. Convex_Area ‚Äì Number of pixels of the smallest convex shell at the region formed by the\n",
    "pumpkin seed.\n",
    "6. Equiv_Diameter ‚Äì Computed as !4ùëé‚ÅÑùúã, where ùëé is the area of the pumpkin seed.\n",
    "7. Eccentricity ‚Äì Eccentricity of a pumpkin seed\n",
    "8. Solidity ‚Äì Convex condition of the pumpkin seeds\n",
    "9. Extent ‚Äì Ratio of a pumpkin seed area to the bounding box pixels\n",
    "10. Roundness ‚Äì Ovality of pumpkin seeds without considering the distortion of the edges.\n",
    "11. Aspect_Ration ‚Äì Aspect ratio of the pumpkin seeds\n",
    "12. Compactness ‚Äì Proportion of the area of the pumpkin seed relative to the area of the circle\n",
    "with the same circumference\n",
    "13. Class ‚Äì Either Cercevelik or Urgup Sivrisi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 3\n",
    "\n",
    "### List of Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 4\n",
    "\n",
    "### Data preprocessing and cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['56276', '888.242', '326.1485', ..., '1.4809', '0.8207', '0'],\n",
       "       ['76631', '1068.146', '417.1932', ..., '1.7811', '0.7487', '0'],\n",
       "       ['71623', '1082.987', '435.8328', ..., '2.0651', '0.6929', '0'],\n",
       "       ...,\n",
       "       ['87994', '1210.314', '507.22', ..., '2.2828', '0.6599', '1'],\n",
       "       ['80011', '1182.947', '501.9065', ..., '2.4513', '0.6359', '1'],\n",
       "       ['84934', '1159.933', '462.8951', ..., '1.9735', '0.7104', '1']],\n",
       "      dtype='<U11')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "with open('pumpkin_seeds.csv', 'r', encoding='utf-8', errors='replace') as csv_file:\n",
    "    raw_data = csv.reader(csv_file)\n",
    "\n",
    "    #Skip headers\n",
    "    next(raw_data)\n",
    "\n",
    "    #Store data into data array\n",
    "    for row in raw_data:\n",
    "        row_data = []\n",
    "        for i in range(13): #Convert errors into 1 or 2 (depending on their specie)\n",
    "            if i == 12 and row[i] == 'ÔøΩerÔøΩevelik':\n",
    "                row_data.append(int(0))\n",
    "            elif i == 12 and row[i] == 'ÔøΩrgÔøΩp Sivrisi':\n",
    "                row_data.append(int(1))\n",
    "            else:\n",
    "                row_data.append(row[i])\n",
    "\n",
    "        data.append(row_data)\n",
    "\n",
    "#Convert data into numpy array\n",
    "np_data = np.array(data)\n",
    "np_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to address the encoding issues, especially in the \"Class\" column. The unique values in the \"Class\" column are showing encoding issues, as evidenced by the presence of escape characters like \\x82. These values are intended to represent the two species of pumpkin seeds. To correct this, we need to replace these incorrectly encoded strings with a correct format. We replaced the string class names with an integer value of 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scaling\n",
    "\n",
    "The numerical features are scaled using `StandardScaler` from `sklearn.preprocessing`. This ensures that all features have a mean of 0 and a standard deviation of 1, which is particularly important for many machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Replace 'original_column_names' with the actual list of column names\n",
    "original_column_names = ['area', 'perimeter', 'major_axis_length', 'minor_axis_length', \n",
    "                         'convex_area', 'equiv_diameter', 'eccentricity', 'solidity', \n",
    "                         'extent', 'roundness', 'aspect_Ration', 'compactness', 'class']\n",
    "\n",
    "# Convert the numpy array to a pandas DataFrame using the original column names\n",
    "pumpkin_seeds_data = pd.DataFrame(np_data, columns=original_column_names)\n",
    "\n",
    "# Selecting only the numerical features for scaling\n",
    "numerical_features = pumpkin_seeds_data.iloc[:, :-1]\n",
    "\n",
    "# Initializing the Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scaling the numerical features\n",
    "scaled_numerical_features = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Creating a new DataFrame with scaled values using the original column names (excluding 'Class')\n",
    "scaled_numerical_df = pd.DataFrame(scaled_numerical_features, columns=original_column_names[:-1])\n",
    "\n",
    "# Adding the non-numerical column ('Class') back to the DataFrame\n",
    "scaled_pumpkin_seeds_data = pd.concat([scaled_numerical_df, pumpkin_seeds_data['class']], axis=1)\n",
    "\n",
    "scaled_pumpkin_seeds_data\n",
    "\n",
    "# Creating the feature set 'X' and the target 'y'\n",
    "X = scaled_pumpkin_seeds_data.iloc[:, :-1]  # All columns except the last one\n",
    "y = scaled_pumpkin_seeds_data['class']      # Only the last column\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "## SECTION 5 - Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "## SECTION 6 - Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 13)\n",
      "testbryce\n"
     ]
    }
   ],
   "source": [
    "print(np_data.shape)\n",
    "print(\"testbryce\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Logistic Regression model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "logi_model = SGDClassifier(loss = 'log_loss', eta0 = 0.001, max_iter = 200, \n",
    "                            learning_rate = 'constant', random_state = 42, verbose = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to numpy array since dataloader (for minibatch gradient descent) expects numpy array \n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and get the predictions\n",
    "logi_model.fit(X_train, y_train)\n",
    "predictions_train = logi_model.predict(X_train)\n",
    "predictions_test = logi_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, actual):\n",
    "    \n",
    "    correct = np.sum(predictions == actual)\n",
    "    \n",
    "    accuracy = correct/len(predictions) * 100.0\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  88.16000000000001 %\n",
      "Testing accuracy:  85.11999999999999 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy: \", compute_accuracy(y_train, predictions_train),\"%\")\n",
    "print(\"Testing accuracy: \", compute_accuracy(y_test, predictions_test),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving the model by using minibatch gradient descent\n",
    "\n",
    "logi_model_bgd = SGDClassifier(loss = 'log_loss', eta0 = 0.001, learning_rate = 'constant', random_state = 42, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use data_loader python file from our past notebooks\n",
    "from data_loader import DataLoader\n",
    "data_loader = DataLoader(X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 0.4595718714356607\n",
      "Epoch: 2 \tLoss: 0.35516606210237706\n",
      "Epoch: 3 \tLoss: 0.3357882392212421\n",
      "Epoch: 4 \tLoss: 0.3279783392659519\n",
      "Epoch: 5 \tLoss: 0.32360040382474337\n",
      "Epoch: 6 \tLoss: 0.321005672417068\n",
      "Epoch: 7 \tLoss: 0.3195198723298882\n",
      "Epoch: 8 \tLoss: 0.3185109131186593\n",
      "Epoch: 9 \tLoss: 0.3177632775753124\n",
      "Epoch: 10 \tLoss: 0.3169705934716691\n",
      "Epoch: 11 \tLoss: 0.3166404296483876\n",
      "Epoch: 12 \tLoss: 0.3161968519421993\n",
      "Epoch: 13 \tLoss: 0.3159738371374772\n",
      "Epoch: 14 \tLoss: 0.315747827507336\n",
      "Epoch: 15 \tLoss: 0.3155693458584706\n",
      "Epoch: 16 \tLoss: 0.315457750447972\n",
      "Epoch: 17 \tLoss: 0.3153374591525945\n",
      "Epoch: 18 \tLoss: 0.31519492246815084\n",
      "Epoch: 19 \tLoss: 0.31512172143226175\n",
      "Epoch: 20 \tLoss: 0.31505562212541716\n",
      "Epoch: 21 \tLoss: 0.31498762217018716\n",
      "Epoch: 22 \tLoss: 0.31491607914036507\n",
      "Epoch: 23 \tLoss: 0.31486102299157664\n",
      "Epoch: 24 \tLoss: 0.3148125730302644\n",
      "Epoch: 25 \tLoss: 0.31478546543035113\n",
      "Epoch: 26 \tLoss: 0.31470970250015495\n",
      "Epoch: 27 \tLoss: 0.31466784649149004\n",
      "Epoch: 28 \tLoss: 0.3146458481246974\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "max_epochs = 200\n",
    "e = 0\n",
    "is_converged = False\n",
    "previous_loss = 0\n",
    "labels = np.unique(y_train)\n",
    "\n",
    "# For each epoch\n",
    "while e < max_epochs and is_converged is not True:\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    X_batch, y_batch = data_loader.get_batch()\n",
    "    \n",
    "    for X, y in zip(X_batch, y_batch):\n",
    "        \n",
    "        # Partial fit the model\n",
    "        logi_model_bgd.partial_fit(X,y,labels)\n",
    "        \n",
    "        # Compute the loss\n",
    "        y_pred = logi_model_bgd.predict_proba(X_train)\n",
    "        loss += log_loss(y_train, y_pred)\n",
    "        \n",
    "    # Display the average loss per epoch\n",
    "    print('Epoch:', e + 1, '\\tLoss:', (loss / len(X_batch)))\n",
    "    \n",
    "    if abs(previous_loss - loss) < 0.005:\n",
    "        is_converged = True\n",
    "    else:\n",
    "        previous_loss = loss\n",
    "        e += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = logi_model_bgd.predict(X_train)\n",
    "predictions_test = logi_model_bgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  88.32 %\n",
      "Testing accuracy:  85.92 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy: \", compute_accuracy(y_train, predictions_train),\"%\")\n",
    "print(\"Testing accuracy: \", compute_accuracy(y_test, predictions_test),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'l1_ratio': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "    'tol': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'eta0': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['none', 'l2', 'l1', 'elasticnet']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_model_hyper = SGDClassifier(loss='log_loss', learning_rate='constant', random_state=42, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search_logi_model = RandomizedSearchCV(estimator=logi_model_hyper, param_distributions=hyperparameters, n_iter=100, cv=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bryce Salvador\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "90 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "90 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bryce Salvador\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Bryce Salvador\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Bryce Salvador\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Bryce Salvador\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of SGDClassifier must be a str among {'l1', 'elasticnet', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Bryce Salvador\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.57706667        nan        nan 0.52       0.48       0.56853333\n",
      "        nan 0.52       0.496      0.81066667 0.8768     0.56213333\n",
      " 0.8448     0.86986667 0.60586667 0.88373333 0.52       0.53813333\n",
      "        nan 0.86773333 0.496      0.8352     0.8832     0.8752\n",
      " 0.86986667 0.86986667 0.8816     0.8816     0.52              nan\n",
      " 0.57173333        nan 0.8768     0.86773333 0.86613333 0.52\n",
      " 0.85973333 0.82453333 0.84533333        nan 0.56213333 0.76053333\n",
      "        nan 0.8784     0.6832     0.85973333 0.8656            nan\n",
      " 0.8704     0.60586667 0.52       0.48       0.87893333 0.7328\n",
      "        nan 0.87626667 0.86773333 0.8704     0.62133333 0.88266667\n",
      "        nan        nan 0.48       0.86613333        nan        nan\n",
      "        nan 0.6976     0.56213333 0.87946667 0.82453333 0.87893333\n",
      " 0.8416     0.70026667 0.48       0.8816            nan 0.87893333\n",
      " 0.52       0.70026667 0.7696     0.87893333 0.85973333        nan\n",
      " 0.48       0.488      0.77813333 0.864      0.52       0.57706667\n",
      " 0.86506667        nan 0.7104     0.48       0.87893333 0.87893333\n",
      " 0.83146667 0.87893333 0.776      0.8352    ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=SGDClassifier(learning_rate=&#x27;constant&#x27;,\n",
       "                                           loss=&#x27;log_loss&#x27;, random_state=42),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={&#x27;alpha&#x27;: [0.0001, 0.001, 0.01, 0.1, 1,\n",
       "                                                  10, 100],\n",
       "                                        &#x27;eta0&#x27;: [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                                        &#x27;l1_ratio&#x27;: [0, 0.1, 0.2, 0.3, 0.4, 0.5,\n",
       "                                                     0.6, 0.7, 0.8, 0.9, 1],\n",
       "                                        &#x27;penalty&#x27;: [&#x27;none&#x27;, &#x27;l2&#x27;, &#x27;l1&#x27;,\n",
       "                                                    &#x27;elasticnet&#x27;],\n",
       "                                        &#x27;tol&#x27;: [0.0001, 0.001, 0.01, 0.1]},\n",
       "                   random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=SGDClassifier(learning_rate=&#x27;constant&#x27;,\n",
       "                                           loss=&#x27;log_loss&#x27;, random_state=42),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={&#x27;alpha&#x27;: [0.0001, 0.001, 0.01, 0.1, 1,\n",
       "                                                  10, 100],\n",
       "                                        &#x27;eta0&#x27;: [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                                        &#x27;l1_ratio&#x27;: [0, 0.1, 0.2, 0.3, 0.4, 0.5,\n",
       "                                                     0.6, 0.7, 0.8, 0.9, 1],\n",
       "                                        &#x27;penalty&#x27;: [&#x27;none&#x27;, &#x27;l2&#x27;, &#x27;l1&#x27;,\n",
       "                                                    &#x27;elasticnet&#x27;],\n",
       "                                        &#x27;tol&#x27;: [0.0001, 0.001, 0.01, 0.1]},\n",
       "                   random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(learning_rate=&#x27;constant&#x27;, loss=&#x27;log_loss&#x27;, random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(learning_rate=&#x27;constant&#x27;, loss=&#x27;log_loss&#x27;, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=SGDClassifier(learning_rate='constant',\n",
       "                                           loss='log_loss', random_state=42),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={'alpha': [0.0001, 0.001, 0.01, 0.1, 1,\n",
       "                                                  10, 100],\n",
       "                                        'eta0': [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                                        'l1_ratio': [0, 0.1, 0.2, 0.3, 0.4, 0.5,\n",
       "                                                     0.6, 0.7, 0.8, 0.9, 1],\n",
       "                                        'penalty': ['none', 'l2', 'l1',\n",
       "                                                    'elasticnet'],\n",
       "                                        'tol': [0.0001, 0.001, 0.01, 0.1]},\n",
       "                   random_state=42)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_logi_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.001, eta0=0.001, l1_ratio=0.4, learning_rate='constant',\n",
      "              loss='log_loss', penalty='l1', random_state=42, tol=0.0001)\n"
     ]
    }
   ],
   "source": [
    "print(random_search_logi_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_model_besthyper = SGDClassifier(alpha=0.001, eta0=0.001, l1_ratio=0.4, learning_rate='constant', loss='log_loss', penalty='l1', random_state=42, tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(alpha=0.001, eta0=0.001, l1_ratio=0.4, learning_rate=&#x27;constant&#x27;,\n",
       "              loss=&#x27;log_loss&#x27;, penalty=&#x27;l1&#x27;, random_state=42, tol=0.0001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(alpha=0.001, eta0=0.001, l1_ratio=0.4, learning_rate=&#x27;constant&#x27;,\n",
       "              loss=&#x27;log_loss&#x27;, penalty=&#x27;l1&#x27;, random_state=42, tol=0.0001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(alpha=0.001, eta0=0.001, l1_ratio=0.4, learning_rate='constant',\n",
       "              loss='log_loss', penalty='l1', random_state=42, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logi_model_besthyper.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = logi_model_besthyper.predict(X_train)\n",
    "predictions_test = logi_model_besthyper.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  88.21333333333334 %\n",
      "Testing accuracy:  85.28 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy: \", compute_accuracy(y_train, predictions_train),\"%\")\n",
    "print(\"Testing accuracy: \", compute_accuracy(y_test, predictions_test),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_model_besthyper_bgd = SGDClassifier(alpha=0.001, eta0=0.001, l1_ratio=0.4, learning_rate='constant', loss='log_loss', penalty='l1', random_state=42, tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 0.4614656530193545\n",
      "Epoch: 2 \tLoss: 0.3574764300866157\n",
      "Epoch: 3 \tLoss: 0.3361700942131213\n",
      "Epoch: 4 \tLoss: 0.3287530326963546\n",
      "Epoch: 5 \tLoss: 0.32362751212378377\n",
      "Epoch: 6 \tLoss: 0.3212425572964651\n",
      "Epoch: 7 \tLoss: 0.31968856564839115\n",
      "Epoch: 8 \tLoss: 0.3185586716237862\n",
      "Epoch: 9 \tLoss: 0.3179358987085836\n",
      "Epoch: 10 \tLoss: 0.31726491661695755\n",
      "Epoch: 11 \tLoss: 0.31683570866980926\n",
      "Epoch: 12 \tLoss: 0.3164973433041607\n",
      "Epoch: 13 \tLoss: 0.3161556643726129\n",
      "Epoch: 14 \tLoss: 0.3159659857940005\n",
      "Epoch: 15 \tLoss: 0.3157938604376914\n",
      "Epoch: 16 \tLoss: 0.31565034641279255\n",
      "Epoch: 17 \tLoss: 0.31558557319229946\n",
      "Epoch: 18 \tLoss: 0.315429836158283\n",
      "Epoch: 19 \tLoss: 0.3153470983209677\n",
      "Epoch: 20 \tLoss: 0.3152556356553599\n",
      "Epoch: 21 \tLoss: 0.31524052887584375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "max_epochs = 200\n",
    "e = 0\n",
    "is_converged = False\n",
    "previous_loss = 0\n",
    "labels = np.unique(y_train)\n",
    "\n",
    "# For each epoch\n",
    "while e < max_epochs and is_converged is not True:\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    X_batch, y_batch = data_loader.get_batch()\n",
    "    \n",
    "    for X, y in zip(X_batch, y_batch):\n",
    "        \n",
    "        # Partial fit the model\n",
    "        logi_model_besthyper_bgd.partial_fit(X,y,labels)\n",
    "        \n",
    "        # Compute the loss\n",
    "        y_pred = logi_model_besthyper_bgd.predict_proba(X_train)\n",
    "        loss += log_loss(y_train, y_pred)\n",
    "        \n",
    "    # Display the average loss per epoch\n",
    "    print('Epoch:', e + 1, '\\tLoss:', (loss / len(X_batch)))\n",
    "    \n",
    "    if abs(previous_loss - loss) < 0.005:\n",
    "        is_converged = True\n",
    "    else:\n",
    "        previous_loss = loss\n",
    "        e += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = logi_model_besthyper_bgd.predict(X_train)\n",
    "predictions_test = logi_model_besthyper_bgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  88.21333333333334 %\n",
      "Testing accuracy:  85.44 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy: \", compute_accuracy(y_train, predictions_train),\"%\")\n",
    "print(\"Testing accuracy: \", compute_accuracy(y_test, predictions_test),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
