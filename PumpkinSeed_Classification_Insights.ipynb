{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1 \n",
    "### Introduction to the problem/task and dataset\n",
    "\n",
    "PumpkinSeed-ML-Insights is a comprehensive repository dedicated to the exploration and analysis of Turkish pumpkin seed varieties, with a focus on classifying whether a seed belongs to Urgup Sivrisi or Cercevelik species. This project demonstrates the knowledge of authors in data science and machine learning.\n",
    "\n",
    "Within this repository, you will find a Jupyter Notebook that serves as a self-explanatory document, guiding you through the entire process. This repository also contains three Python files, each implementing a different machine learning model: `knn_pumpkinseed.py`, `logistic_regression_pumkinseed.py`, and `neural_network_pumpkinseed.py`. There is also the `pumpkin_seeds.csv`, which contains the data and `Pumpkin_seeds.pdf`, which contains some description of the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 2\n",
    "### Description of the dataset\n",
    "\n",
    "`pumpkin_seeds.csv` is a CSV file containing information about Pumpkin Seeds found in Turkey. \n",
    "\n",
    "This dataset came from the study `The use of machine learning methods in classification of pumpkin seeds (Cucurbita pepo L.).` by Koklu, M., Sarigil, S., & Ozbek, O. in 2021. In their paper, they used a product shooting box to obtain quality images of the pumpkin seeds. The authors converted the images to a gray tone and then to binary images. To convert the image data into a CSV file, they extracted 12 morphological features.\n",
    "\n",
    "Overall, the CSV file has 13 columns/features and 2500 rows. The first 12 columns are from the extracted morphological features, while the last column classifies whether it belongs to the Urgup Sivrisi or Cercevelik species. There are 2500 rows, representing a single seed used in the study. There are 1200 Urgup Sivrisi and 1300 Cercevelik species of pumpkin seeds. \n",
    "\n",
    "The features found in this CSV file are as follows:\n",
    "1. Area ‚Äì Number of pixels within the borders of a pumpkin seed\n",
    "2. Perimeter ‚Äì Circumference in pixels of a pumpkin seed\n",
    "3. Major_Axis_Length ‚Äì Large axis distance of a pumpkin seed\n",
    "4. Minor_Axis_Length ‚Äì Small axis distance of a pumpkin seed\n",
    "5. Convex_Area ‚Äì Number of pixels of the smallest convex shell at the region formed by the\n",
    "pumpkin seed.\n",
    "6. Equiv_Diameter ‚Äì Computed as !4ùëé‚ÅÑùúã, where ùëé is the area of the pumpkin seed.\n",
    "7. Eccentricity ‚Äì Eccentricity of a pumpkin seed\n",
    "8. Solidity ‚Äì Convex condition of the pumpkin seeds\n",
    "9. Extent ‚Äì Ratio of a pumpkin seed area to the bounding box pixels\n",
    "10. Roundness ‚Äì Ovality of pumpkin seeds without considering the distortion of the edges.\n",
    "11. Aspect_Ration ‚Äì Aspect ratio of the pumpkin seeds\n",
    "12. Compactness ‚Äì Proportion of the area of the pumpkin seed relative to the area of the circle\n",
    "with the same circumference\n",
    "13. Class ‚Äì Either Cercevelik or Urgup Sivrisi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 3\n",
    "\n",
    "### List of Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SECTION 4\n",
    "\n",
    "### Data preprocessing and cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to address the encoding issues, especially in the \"Class\" column. The unique values in the \"Class\" column are showing encoding issues, as evidenced by the presence of escape characters like \\x82. These values are intended to represent the two species of pumpkin seeds. To correct this, we need to replace these incorrectly encoded strings with a correct format. We replaced the string class names with an integer value of 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "with open('pumpkin_seeds.csv', 'r', encoding='utf-8', errors='replace') as csv_file:\n",
    "    raw_data = csv.reader(csv_file)\n",
    "\n",
    "    #Skip headers\n",
    "    next(raw_data)\n",
    "\n",
    "    #Store data into data array\n",
    "    for row in raw_data:\n",
    "        row_data = []\n",
    "        for i in range(13): #Convert errors into 1 or 2 (depending on their specie)\n",
    "            if i == 12 and row[i] == 'ÔøΩerÔøΩevelik':\n",
    "                row_data.append(int(0))\n",
    "            elif i == 12 and row[i] == 'ÔøΩrgÔøΩp Sivrisi':\n",
    "                row_data.append(int(1))\n",
    "            else:\n",
    "                row_data.append(row[i])\n",
    "\n",
    "        data.append(row_data)\n",
    "\n",
    "#Convert data into numpy array\n",
    "np_data = np.array(data)\n",
    "np_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scaling\n",
    "\n",
    "The numerical features are scaled using `StandardScaler` from `sklearn.preprocessing`. This ensures that all features have a mean of 0 and a standard deviation of 1, which is particularly important for many machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Replace 'original_column_names' with the actual list of column names\n",
    "original_column_names = ['area', 'perimeter', 'major_axis_length', 'minor_axis_length', \n",
    "                         'convex_area', 'equiv_diameter', 'eccentricity', 'solidity', \n",
    "                         'extent', 'roundness', 'aspect_Ration', 'compactness', 'class']\n",
    "\n",
    "# Convert the numpy array to a pandas DataFrame using the original column names\n",
    "pumpkin_seeds_data = pd.DataFrame(np_data, columns=original_column_names)\n",
    "\n",
    "# Selecting only the numerical features for scaling\n",
    "numerical_features = pumpkin_seeds_data.iloc[:, :-1]\n",
    "\n",
    "# Initializing the Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scaling the numerical features\n",
    "scaled_numerical_features = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Creating a new DataFrame with scaled values using the original column names (excluding 'class')\n",
    "scaled_numerical_df = pd.DataFrame(scaled_numerical_features, columns=original_column_names[:-1])\n",
    "\n",
    "# Adding the non-numerical column ('class') back to the DataFrame\n",
    "scaled_pumpkin_seeds_data = pd.concat([scaled_numerical_df, pumpkin_seeds_data['class']], axis=1)\n",
    "\n",
    "scaled_pumpkin_seeds_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the data, it is now time to define our X and y variables. The X variable will hold the features, while the y variable contains our target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the feature set 'X' and the target 'y'\n",
    "X = scaled_pumpkin_seeds_data.iloc[:, :-1]  # All columns except the last one\n",
    "y = scaled_pumpkin_seeds_data['class']      # Only the last column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate the training from the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "print('X_train shape', X_train.shape)\n",
    "print('y_train shape', X_test.shape)\n",
    "print('X_test shape', y_train.shape)\n",
    "print('y_test shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "## SECTION 5 - Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics\n",
    "Here, we examine the descriptive statistics of the numerical features. This includes measures like mean, median, standard deviation, and quartiles, which provide insights into the central tendency and spread of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistics for Numerical Features\n",
    "descriptive_stats = scaled_pumpkin_seeds_data.describe()\n",
    "descriptive_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the Distribution of Each Feature\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = scaled_pumpkin_seeds_data.columns[:-1]  # Excluding the class column\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(scaled_pumpkin_seeds_data[feature], bins=20, alpha=0.7)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plots for Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plots for Each Feature\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x=scaled_pumpkin_seeds_data[feature])\n",
    "    plt.title(f'Box Plot of {feature}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "## SECTION 6 - Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "The K-Nearest Neighbors (KNN) algorithm is often used for classification task. It clasifies data points based on the majority class of their k-nearest neighbors. While it is a simple algorithm, one of its advantages is that it does not make strong assumptions about the underlying data distribution. This makes it versatile and suitable for various types of datasets, including this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the KNN classifier\n",
    "model = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model on the training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy of the model on the test data\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print('Accuracy:', accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting arbitrary value (5) for the number of neighbors\n",
    "neighbors = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Neighbors\n",
    "This hyperparameter is crucial in KNN which represent the number of neighbors to consider when making predcitions.\n",
    "\n",
    "The value assigned to the variable neighbors is chosen arbitrarily to assess the model's performance improvement following the cross-validation of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, data_index = model.kneighbors(X_test,neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top neighbors of the test data\n",
    "np.squeeze(distances)\n",
    "np.squeeze(data_index)\n",
    "print('Top 5 neighbors of the test data:')\n",
    "for i in range(5):\n",
    "    print('Neighbor:', i+1)\n",
    "    print('Distance:', distances[i])\n",
    "    print('Data index:', data_index[i])\n",
    "    print('Class:', y_train.iloc[data_index[i]])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cross validation libraries\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = 5\n",
    "k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]\n",
    "scores = np.zeros((len(k_choices), k_folds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "These will be the values used for cross-validation. Cross-validation is used to assess the model's performance across different folds.\n",
    "\n",
    "Setting `k_folds = 5` means that the dataset will be divided into 5 roughly equal-sized subsets. This is to be able to assess the performance of the model more reliably. The given value is a good balance between reducing variability in the estimate and keeping computational cost reasonable.\n",
    "\n",
    "Unlike other algorithms, KNN does not involve a learning process like other iterative algorithms. This model directly memorizes the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k_choices)):\n",
    "    model = KNeighborsClassifier(n_neighbors=k_choices[i])\n",
    "    scores[i] = cross_val_score(model, X_train, y_train, cv=k_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(scores):\n",
    "    for i in range(len(scores)):\n",
    "        x=[k_choices[i]] * 5\n",
    "        plt.scatter(x, scores[i])\n",
    "        \n",
    "plot_scatter(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average scores of each fold\n",
    "avg_scores = np.mean(scores, axis=1)\n",
    "print('Average scores:', avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average accuracy for each k\n",
    "stddev_scores = np.std(scores, axis=1)\n",
    "print('Standard deviation of scores:', stddev_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(scores)\n",
    "\n",
    "plt.errorbar(k_choices, avg_scores, yerr=stddev_scores)\n",
    "plt.title('Cross-validation on k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Cross-validation accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on selected k value\n",
    "model = KNeighborsClassifier(n_neighbors=15)\n",
    "\n",
    "# Training the model on the training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy of the model on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print('Accuracy:', accuracy*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (KNN)\n",
    "The accuracy of the model demonstrates improvement as the number of neighbors increases, peaking at 15. This particular value of `n_neighbors` yields the highest average accuracy, indicating a balanced trade-off between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "The second model we are using is `Logistic Regression`, a binary classifier. Within the context of our dataset, two distinct classes are identified for the pumpkin seeds data, namely Cercevelik and Urgup Sivrisi. The rationale behind selecting logistic regression as a suitable machine learning model for this binary classification problem lies in its capacity to predict the probabilities associated with a given data point belonging to a particular class.\n",
    "\n",
    "For this model, we will be using the `SGDClassifier`. We are setting the loss parameter to be `log_loss` to use the loss function of a Logistic Regression model. Based from its name, this model uses a stochastic gradient descent as its optimizer. The initial learning rate is `0.001`, which can be altered later after tuning. The learning rate schedule we are using is `constant`, which means we are always using `0.001` to find the minimum of the loss function throughout the training process.\n",
    "\n",
    "For comparison purposes, we have also incorporated a `mini-batch gradient descent` optimization algorithm to facilitate the training of the logistic regression model. This iterative optimization technique enhances the efficiency of parameter updates by processing a subset, or mini-batch, of the entire dataset at each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Logistic Regression model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "logi_model = SGDClassifier(loss = 'log_loss', eta0 = 0.001, max_iter = 200, \n",
    "                            learning_rate = 'constant', random_state = 42, verbose = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to numpy array since dataloader (for minibatch gradient descent) expects numpy array \n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and get the predictions\n",
    "logi_model.fit(X_train, y_train)\n",
    "predictions_train = logi_model.predict(X_train)\n",
    "predictions_test = logi_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, actual):\n",
    "    \n",
    "    correct = np.sum(predictions == actual)\n",
    "    \n",
    "    accuracy = correct/len(predictions) * 100.0\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy: \", compute_accuracy(y_train, predictions_train),\"%\")\n",
    "print(\"Testing accuracy: \", compute_accuracy(y_test, predictions_test),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using minibatch gradient descent instead of stochastic gradient descent\n",
    "logi_model_bgd = SGDClassifier(loss = 'log_loss', eta0 = 0.001, learning_rate = 'constant', random_state = 42, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use data_loader python file from our past notebooks\n",
    "from data_loader import DataLoader\n",
    "data_loader = DataLoader(X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "max_epochs = 200\n",
    "e = 0\n",
    "is_converged = False\n",
    "previous_loss = 0\n",
    "labels = np.unique(y_train)\n",
    "\n",
    "# For each epoch\n",
    "while e < max_epochs and is_converged is not True:\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    X_batch, y_batch = data_loader.get_batch()\n",
    "    \n",
    "    for X, y in zip(X_batch, y_batch):\n",
    "        \n",
    "        # Partial fit the model\n",
    "        logi_model_bgd.partial_fit(X,y,labels)\n",
    "        \n",
    "        # Compute the loss\n",
    "        y_pred = logi_model_bgd.predict_proba(X_train)\n",
    "        loss += log_loss(y_train, y_pred)\n",
    "        \n",
    "    # Display the average loss per epoch\n",
    "    print('Epoch:', e + 1, '\\tLoss:', (loss / len(X_batch)))\n",
    "    \n",
    "    if abs(previous_loss - loss) < 0.005:\n",
    "        is_converged = True\n",
    "    else:\n",
    "        previous_loss = loss\n",
    "        e += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = logi_model_bgd.predict(X_train)\n",
    "predictions_test = logi_model_bgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy: \", compute_accuracy(y_train, predictions_train),\"%\")\n",
    "print(\"Testing accuracy: \", compute_accuracy(y_test, predictions_test),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (Logistic Regression)\n",
    "\n",
    "To fine tune our model, we plan to have these hyperparameter options be assessed by `RandomizedSearchCV`.\n",
    "\n",
    "These hyperparameters are relevant in our `SGDClassifier` using `Logistic Regression`.\n",
    "\n",
    "1. `alpha` is the regularization parameter strength. This helps in preventing overfitting the model to the training data. The greater the alpha, the stronger the regularization. However, there is a possibility that if alpha is too big, it can lead to underfitting.\n",
    "\n",
    "2. `l1_ratio` controls the balance between `l1` and `l2` regularization. A value of 0 means it's using `l2` regularization and a value of 1 means it's using `l1` regularization. \n",
    "\n",
    "3. `tol` is the tolerance for the stopping criterion. It serves as the threshold at which the algorithm will stop iterating.\n",
    "\n",
    "4. `eta0` is the initial learning rate. It controls the step size at each iteration. Since we're using `constant` as our learning_rate schedule, the value of `eta0` never changes.\n",
    "\n",
    "5. `penalty` is the regularization term to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'l1_ratio': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "    'tol': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'eta0': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['none', 'l2', 'l1', 'elasticnet']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logi_model_hyper = SGDClassifier(loss='log_loss', learning_rate='constant', random_state=42, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search_logi_model = RandomizedSearchCV(estimator=logi_model_hyper, param_distributions=hyperparameters, n_iter=100, cv=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_logi_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_search_logi_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SGDClassifier using the best estimators found by RandomizedSearchCV\n",
    "logi_model_besthyper = SGDClassifier(alpha=0.001, eta0=0.001, l1_ratio=0.4, learning_rate='constant', loss='log_loss', penalty='l1', random_state=42, tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and get the predictions\n",
    "logi_model_besthyper.fit(X_train, y_train)\n",
    "predictions_train = logi_model_besthyper.predict(X_train)\n",
    "predictions_test = logi_model_besthyper.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy: \", compute_accuracy(y_train, predictions_train),\"%\")\n",
    "print(\"Testing accuracy: \", compute_accuracy(y_test, predictions_test),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of our model without hyperparameter tuning is as follows:  \n",
    "Training accuracy:  `88.16000000000001 %`  \n",
    "Testing accuracy:  `85.11999999999999 %`\n",
    "\n",
    "After tuning, we got:  \n",
    "Training accuracy:  `88.21333333333334 %`  \n",
    "Testing accuracy:  `85.28 %`\n",
    "\n",
    "Hyperparameter tuning has shown an improvement in our accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using minibatch gradient descent instead of stochastic gradient descent\n",
    "logi_model_besthyper_bgd = SGDClassifier(alpha=0.001, eta0=0.001, l1_ratio=0.4, learning_rate='constant', loss='log_loss', penalty='l1', random_state=42, tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "max_epochs = 200\n",
    "e = 0\n",
    "is_converged = False\n",
    "previous_loss = 0\n",
    "labels = np.unique(y_train)\n",
    "\n",
    "# For each epoch\n",
    "while e < max_epochs and is_converged is not True:\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    X_batch, y_batch = data_loader.get_batch()\n",
    "    \n",
    "    for X, y in zip(X_batch, y_batch):\n",
    "        \n",
    "        # Partial fit the model\n",
    "        logi_model_besthyper_bgd.partial_fit(X,y,labels)\n",
    "        \n",
    "        # Compute the loss\n",
    "        y_pred = logi_model_besthyper_bgd.predict_proba(X_train)\n",
    "        loss += log_loss(y_train, y_pred)\n",
    "        \n",
    "    # Display the average loss per epoch\n",
    "    print('Epoch:', e + 1, '\\tLoss:', (loss / len(X_batch)))\n",
    "    \n",
    "    if abs(previous_loss - loss) < 0.005:\n",
    "        is_converged = True\n",
    "    else:\n",
    "        previous_loss = loss\n",
    "        e += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = logi_model_besthyper_bgd.predict(X_train)\n",
    "predictions_test = logi_model_besthyper_bgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy: \", compute_accuracy(y_train, predictions_train),\"%\")\n",
    "print(\"Testing accuracy: \", compute_accuracy(y_test, predictions_test),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of our model in mini-batch gradient descent without hyperparameter tuning is as follows:  \n",
    "Training accuracy:  `88.32 %`  \n",
    "Testing accuracy:  `85.92 %`\n",
    "\n",
    "After tuning, we got:  \n",
    "Training accuracy:  `88.21333333333334 %`  \n",
    "Testing accuracy:  `85.44 %`\n",
    "\n",
    "Unfortunately, in mini-batch gradient descent, the hyperparameter tuning did worse. A possible reason for this is the RandomizedSearchCV class only accounted for stochastic gradient descent and not for batch gradient descent.\n",
    "\n",
    "\n",
    "The batch size used in the data loader is 10. A higher batch size resulted to the model performing worse. Lowering the batch size made the model perform better, but made it slower. \n",
    "\n",
    "The accuracies when the batch size used in the data loader is 5 are as follows:\n",
    "\n",
    "Training accuracy:  `88.37333333333333 %`  \n",
    "Testing accuracy:  `86.08 %`\n",
    "\n",
    "Training accuracy:  `88.21333333333334 % `  \n",
    "Testing accuracy:  `85.92 %`\n",
    "\n",
    "As you can see, the performance is similar. The only issue is the speed. Therefore, we set the batch_size to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
